{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSeek-R1-Distill-Qwen-1.5B Fine-tuning (Fixed)\n",
    "\n",
    "Fixes applied vs original notebook:\n",
    "1. **ChatML format** with `<|im_start|>`/`<|im_end|>` tokens (Qwen model family)\n",
    "2. **`<think>` tags** for `reasoning_content` (R1-distill style)\n",
    "3. **Label masking**: prompt tokens set to `-100`, only train on assistant response\n",
    "4. **EOS token** included via `<|im_end|>`\n",
    "5. **Dynamic padding** via DataCollator (not padding every sample to max_length)\n",
    "6. **max_length** increased to 2048 (was 512)\n",
    "7. **num_train_epochs** increased to 3 (was 1)\n",
    "8. **LoRA r=16, alpha=32** (was r=8, alpha=16)\n",
    "9. **LoRA applied to all linear layers** (attention + MLP)\n",
    "10. **Warmup + cosine** learning rate scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "DATA_FILE = \"distill_psychology-10k-r1.json\"\n",
    "OUTPUT_DIR = \"./finetuned_models\"\n",
    "MAX_LENGTH = 2048\n",
    "NUM_EPOCHS = 3\n",
    "LORA_R = 16\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "LEARNING_RATE = 2e-5\n",
    "BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 16\n",
    "INCLUDE_REASONING = True  # set False to train without <think> reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(f\"Tokenizer loaded. Vocab size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Prepare Dataset with ChatML Format\n",
    "\n",
    "Data format per sample:\n",
    "```\n",
    "<|im_start|>user\n",
    "{input}<|im_end|>\n",
    "<|im_start|>assistant\n",
    "<think>\n",
    "{reasoning_content}\n",
    "</think>\n",
    "\n",
    "{content}<|im_end|>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\n\nsamples = []\nskipped = 0\n\nwith open(DATA_FILE, \"r\", encoding=\"utf-8\") as f:\n    raw = f.read()\n\n# Try parsing as a JSON array first, then fall back to JSONL\ntry:\n    data_list = json.loads(raw)\n    print(\"Parsed as JSON array\")\nexcept json.JSONDecodeError:\n    # Parse as JSONL (one JSON object per line)\n    data_list = []\n    for i, line in enumerate(raw.splitlines()):\n        line = line.strip()\n        if not line:\n            continue\n        try:\n            data_list.append(json.loads(line))\n        except json.JSONDecodeError as e:\n            skipped += 1\n            if skipped <= 3:\n                print(f\"Skipping line {i+1}: {e}\")\n    print(f\"Parsed as JSONL ({skipped} lines skipped)\")\n\nfor item in data_list:\n    user_msg = item[\"input\"]\n    response = item[\"content\"]\n    reasoning = item.get(\"reasoning_content\", \"\")\n\n    # Build assistant message with optional <think> block\n    if INCLUDE_REASONING and reasoning:\n        assistant_msg = f\"<think>\\n{reasoning}\\n</think>\\n\\n{response}\"\n    else:\n        assistant_msg = response\n\n    samples.append({\n        \"user\": user_msg,\n        \"assistant\": assistant_msg,\n    })\n\n# Write processed JSONL\nwith open(\"dataset_processed.jsonl\", \"w\", encoding=\"utf-8\") as f:\n    for sample in samples:\n        f.write(json.dumps(sample, ensure_ascii=False) + \"\\n\")\n\nprint(f\"Loaded {len(samples)} samples\")\nprint(f\"Sample 0 user: {samples[0]['user'][:80]}...\")\nprint(f\"Sample 0 assistant: {samples[0]['assistant'][:80]}...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"json\", data_files={\"train\": \"dataset_processed.jsonl\"}, split=\"train\"\n",
    ")\n",
    "split = dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split[\"train\"]\n",
    "eval_dataset = split[\"test\"]\n",
    "\n",
    "print(f\"Total: {len(dataset)}\")\n",
    "print(f\"Train: {len(train_dataset)}\")\n",
    "print(f\"Eval:  {len(eval_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Tokenization with Label Masking\n",
    "\n",
    "**Key fixes:**\n",
    "- Prompt tokens are masked with `-100` in labels (model only learns the assistant response)\n",
    "- **No `padding=max_length`** here — padding is handled dynamically per-batch by the DataCollator (much faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"\n",
    "    Tokenize with ChatML format and mask prompt tokens in labels.\n",
    "    Only the assistant's response contributes to the training loss.\n",
    "    NO padding here — DataCollator will pad dynamically per batch.\n",
    "    \"\"\"\n",
    "    all_input_ids = []\n",
    "    all_attention_mask = []\n",
    "    all_labels = []\n",
    "\n",
    "    for user_msg, assistant_msg in zip(examples[\"user\"], examples[\"assistant\"]):\n",
    "        # Full ChatML-formatted text\n",
    "        full_text = (\n",
    "            f\"<|im_start|>user\\n{user_msg}<|im_end|>\\n\"\n",
    "            f\"<|im_start|>assistant\\n{assistant_msg}<|im_end|>\"\n",
    "        )\n",
    "\n",
    "        # Prompt portion (everything before the assistant's actual content)\n",
    "        prompt_text = (\n",
    "            f\"<|im_start|>user\\n{user_msg}<|im_end|>\\n\"\n",
    "            f\"<|im_start|>assistant\\n\"\n",
    "        )\n",
    "\n",
    "        # Tokenize full text — truncate but do NOT pad\n",
    "        full_tokens = tokenizer(\n",
    "            full_text,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            padding=False,\n",
    "        )\n",
    "\n",
    "        # Tokenize prompt-only to get its token length\n",
    "        prompt_tokens = tokenizer(\n",
    "            prompt_text,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            add_special_tokens=False,\n",
    "        )\n",
    "        prompt_len = len(prompt_tokens[\"input_ids\"])\n",
    "\n",
    "        # Build labels: -100 for prompt tokens (only train on assistant response)\n",
    "        labels = full_tokens[\"input_ids\"][:]\n",
    "        for i in range(min(prompt_len, len(labels))):\n",
    "            labels[i] = -100\n",
    "\n",
    "        all_input_ids.append(full_tokens[\"input_ids\"])\n",
    "        all_attention_mask.append(full_tokens[\"attention_mask\"])\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": all_input_ids,\n",
    "        \"attention_mask\": all_attention_mask,\n",
    "        \"labels\": all_labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tokenizing train dataset...\")\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=train_dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"Tokenizing eval dataset...\")\n",
    "tokenized_eval = eval_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=eval_dataset.column_names\n",
    ")\n",
    "\n",
    "print(f\"Tokenized train: {tokenized_train}\")\n",
    "print(f\"Tokenized eval:  {tokenized_eval}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: verify label masking and token length distribution\n",
    "sample_labels = tokenized_train[0][\"labels\"]\n",
    "sample_ids = tokenized_train[0][\"input_ids\"]\n",
    "n_masked = sum(1 for l in sample_labels if l == -100)\n",
    "n_trained = len(sample_labels) - n_masked\n",
    "\n",
    "print(f\"Sample 0 stats:\")\n",
    "print(f\"  Total tokens:  {len(sample_ids)}\")\n",
    "print(f\"  Trained on:    {n_trained} tokens (assistant response)\")\n",
    "print(f\"  Masked (-100): {n_masked} tokens (prompt)\")\n",
    "\n",
    "# Show token length distribution\n",
    "lengths = [len(x) for x in tokenized_train[\"input_ids\"]]\n",
    "print(f\"\\nToken length distribution (train):\")\n",
    "print(f\"  Min: {min(lengths)}, Max: {max(lengths)}, Avg: {sum(lengths)/len(lengths):.0f}\")\n",
    "print(f\"  Truncated to {MAX_LENGTH}: {sum(1 for l in lengths if l == MAX_LENGTH)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Load Model with 8-bit Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=quantization_config,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "print(\"Model loaded with 8-bit quantization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Apply LoRA (attention + MLP layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=LORA_R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                     \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Training\n",
    "\n",
    "Uses `DataCollatorForSeq2Seq` for **dynamic padding** — each batch is only padded to the longest sample in that batch, not to `max_length`. This is much faster than static padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n",
    "\n",
    "# Dynamic padding: pads each batch to the longest sample in that batch\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,\n",
    "    pad_to_multiple_of=8,  # for GPU efficiency\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    logging_first_step=True,\n",
    "    save_steps=100,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_ratio=0.05,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"Training args configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Save LoRA Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adapter_path = f\"{OUTPUT_DIR}/lora_adapter\"\n",
    "model.save_pretrained(adapter_path)\n",
    "tokenizer.save_pretrained(adapter_path)\n",
    "print(f\"LoRA adapter saved to {adapter_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Merge LoRA into Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "print(\"Loading base model for merging...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"auto\")\n",
    "\n",
    "print(\"Merging LoRA adapter...\")\n",
    "merged_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "merged_model = merged_model.merge_and_unload()\n",
    "\n",
    "merged_path = f\"{OUTPUT_DIR}/merged_model\"\n",
    "merged_model.save_pretrained(merged_path)\n",
    "tokenizer.save_pretrained(merged_path)\n",
    "print(f\"Merged model saved to {merged_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Test Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "test_model = AutoModelForCausalLM.from_pretrained(merged_path).to(\"cuda\")\n",
    "test_tokenizer = AutoTokenizer.from_pretrained(merged_path)\n",
    "\n",
    "test_prompts = [\n",
    "    \"我最近感到非常愤怒，但不知道原因是什么。\",\n",
    "    \"我对所承担的所有责任感到非常不堪重负。\",\n",
    "    \"我晚上总是睡不好觉，白天也没有精神。\",\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    # Use ChatML format for inference (must match training format)\n",
    "    chat_input = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    inputs = test_tokenizer(chat_input, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = test_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            eos_token_id=test_tokenizer.convert_tokens_to_ids(\"<|im_end|>\"),\n",
    "        )\n",
    "\n",
    "    response = test_tokenizer.decode(\n",
    "        outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True\n",
    "    )\n",
    "    print(f\"问: {prompt}\")\n",
    "    print(f\"答: {response}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Interactive Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Interactive chat started. Type 'quit' to exit.\\n\")\n",
    "\n",
    "while True:\n",
    "    prompt = input(\"问: \")\n",
    "    if prompt.strip().lower() == \"quit\":\n",
    "        print(\"再见！\")\n",
    "        break\n",
    "\n",
    "    chat_input = f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
    "    inputs = test_tokenizer(chat_input, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = test_model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            eos_token_id=test_tokenizer.convert_tokens_to_ids(\"<|im_end|>\"),\n",
    "        )\n",
    "\n",
    "    response = test_tokenizer.decode(\n",
    "        outputs[0][inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True\n",
    "    )\n",
    "    print(f\"答: {response}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}