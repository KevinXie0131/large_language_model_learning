ä¸ºäº†æ–¹ä¾¿å®‰è£…ç›´æ¥ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤ï¼š
pip install git+https://github.com/deepseek-ai/Janus
ğŸ® ä¸ºäº†ä¾¿äºä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼Œå¯ä»¥ä½¿ç”¨model_scope

pip install modelscope
å‚è€ƒäº†modelscopeç¤¾åŒºä¸Šçš„æ¡ˆä¾‹

ğŸ” åŠ è½½deepseek- Janus-7Bå¤šæ¨¡æ€æ¨¡å‹
# todo å›¾ç‰‡ç†è§£

import torch
from transformers import AutoModelForCausalLM
from janus.models import MultiModalityCausalLM, VLChatProcessor
from janus.utils.io import load_pil_images
from modelscope import snapshot_download

# specify the path to the model
# model_path = snapshot_download("deepseek-ai/Janus-Pro-7B", local_dir="./model")
model_path = "/data/ms-swift/output/v2-20250222-140230/checkpoint-3"

print("æ¨¡å‹è·¯å¾„ï¼š", model_path)

vl_chat_processor: VLChatProcessor = VLChatProcessor.from_pretrained(model_path)
tokenizer = vl_chat_processor.tokenizer

vl_gpt: MultiModalityCausalLM = AutoModelForCausalLM.from_pretrained(
    model_path, trust_remote_code=True
)
vl_gpt = vl_gpt.to(torch.bfloat16).cuda().eval()

question = "æè¿°ä¸€ä¸‹å›¾ç‰‡å†…å®¹"

image = "./data/images/mapo-tofu.png"
conversation = [
    {
        "role": "<|User|>",
        "content": f"<image_placeholder>\n{question}",
        "images": [image],
    },
    {"role": "<|Assistant|>", "content": ""},
]

# load images and prepare for inputs
pil_images = load_pil_images(conversation)
prepare_inputs = vl_chat_processor(
    conversations=conversation, images=pil_images, force_batchify=True
).to(vl_gpt.device)

# # run image encoder to get the image embeddings
inputs_embeds = vl_gpt.prepare_inputs_embeds(**prepare_inputs)

# # run the model to get the response
outputs = vl_gpt.language_model.generate(
    inputs_embeds=inputs_embeds,
    attention_mask=prepare_inputs.attention_mask,
    pad_token_id=tokenizer.eos_token_id,
    bos_token_id=tokenizer.bos_token_id,
    eos_token_id=tokenizer.eos_token_id,
    max_new_tokens=512,
    do_sample=False,
    use_cache=True,
)

answer = tokenizer.decode(outputs[0].cpu().tolist(), skip_special_tokens=True)
print(f"{prepare_inputs['sft_format'][0]}", answer)
ä¸‹é¢å¼€å§‹å¾®è°ƒï¼ˆå›¾åƒç†è§£çš„å¾®è°ƒè®­ç»ƒï¼‰ ms-swift

å‡†å¤‡ç¯å¢ƒ ğŸš’
git clone https://github.com/modelscope/ms-swift.git
cd ms-swiftc
pip install -e .
å¾®è°ƒå‘½ä»¤ï¼š

å¾®è°ƒå·¥å…·ä½¿ç”¨ğŸ…

CUDA_VISIBLE_DEVICES=0 \
swift sft \
    --model /data/janus-sft/model/deepseek-ai/Janus-Pro-7B \
    --dataset AI-ModelScope/LaTeX_OCR:human_handwrite#50 \
    --train_type lora \
    --torch_dtype bfloat16 \
    --num_train_epochs 1 \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 1 \
    --learning_rate 1e-4 \
    --lora_rank 8 \
    --lora_alpha 32 \
    --target_modules all-linear \
    --freeze_vit true \
    --gradient_accumulation_steps 16 \
    --eval_steps 100 \
    --save_steps 100 \
    --save_total_limit 2 \
    --logging_steps 5 \
    --max_length 2048 \
    --output_dir output \
    --warmup_ratio 0.05 \
    --dataloader_num_workers 4 \
    --dataset_num_proc 4
å½“ç„¶å¯ä»¥é¢„è§ˆæ•°æ®é›†ï¼Œè®¿é—®å¦‚ä¸‹åœ°å€ï¼š

ç‚¹å‡»è®¿é—®

æˆ–è€…å¯ä»¥ä½¿ç”¨å¦‚ä¸‹pythonä»£ç åŠ è½½æ•°æ®é›†ï¼Œè¿›è¡Œé¢„è§ˆï¼š

from modelscope import MsDataset
train_dataset = MsDataset.load("AI-ModelScope/LaTeX_OCR", subset_name="small", split="train")
train_dataset[2]
#{'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=200x50 at 0x15A5D6CE210>,
#'text': '\\rho _ { L } ( q ) = \\sum _ { m = 1 } ^ { L } \\ P _ { L } ( m ) \\ { \\frac { 1 } { q ^ { m - 1 } #} } .'}
len(train_dataset)
#50

æ¨ç†
  CUDA_VISIBLE_DEVICES=0 \
  swift infer \
      --adapters /data/ms-swift/output/v4-20250223-124941/checkpoint-3 \
      --stream false \
      --max_batch_size 1 \
      --load_data_args true \
      --max_new_tokens 2048
ä¿å­˜æ¨¡å‹â€“(æ¨é€åˆ°modelscope)
CUDA_VISIBLE_DEVICES=0 \
swift export \
    --adapters /data/ms-swift/output/v4-20250223-124941/checkpoint-3 \
    --push_to_hub true \
    --hub_model_id 'ä½ çš„æ¨¡å‹id' \
    --hub_token 'ä½ çš„token'